{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import data_loader as data_loaders\n",
    "import model as models\n",
    "import trainer.loss as loss_functions\n",
    "import trainer.metric as metric_functions\n",
    "\n",
    "import utils.util as util\n",
    "from utils import color_print as cp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model cifar100_40/sigmoid_bce_loss/0/cifar100_base/0405_051920/model_best.pth\n",
      "class 0 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/0/0405_074201/model_best.pth\n",
      "class 1 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/1/0405_074304/model_best.pth\n",
      "class 2 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/2/0405_074327/model_best.pth\n",
      "class 3 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/3/0405_074346/model_best.pth\n",
      "class 4 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/4/0405_074441/model_best.pth\n",
      "class 5 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/5/0405_074510/model_best.pth\n",
      "class 6 model cifar100_40/sigmoid_bce_loss/0/cifar100_fine_tune/6/0405_074535/model_best.pth\n"
     ]
    }
   ],
   "source": [
    "# task = \"kws_res15_narrow\"\n",
    "task = \"cifar100\"\n",
    "eval_target_class = list(np.arange(7))\n",
    "\n",
    "base_model_config = 'config/{}_base.json'.format(task)\n",
    "fine_model_config = 'config/{}_fine_tune.json'.format(task)\n",
    "\n",
    "# folder_name = 'dev'\n",
    "# folder_name = 'cifar100_40/logsoftmax_nll_loss/0'\n",
    "folder_name = 'cifar100_40/sigmoid_bce_loss/0'\n",
    "# folder_name = 'cifar100_40/softmax_bce_loss/0'\n",
    "most_recent_model = max(os.listdir(folder_name+\"/\"+task+\"_base\"))\n",
    "base_model_cpt = os.path.join(folder_name, task+'_base', most_recent_model, 'model_best.pth')\n",
    "print('base_model', base_model_cpt)\n",
    "\n",
    "cpts = {}\n",
    "\n",
    "for i in eval_target_class:\n",
    "    most_recent_model = max(os.listdir(folder_name+\"/\"+task+\"_fine_tune/\"+str(i)+\"/\"))\n",
    "    cpts[i] = os.path.join(folder_name, task+'_fine_tune/'+str(i)+'/', most_recent_model, 'model_best.pth')\n",
    "    print('class '+str(i)+' model', cpts[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_all_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = torch.load(base_model_cpt)['config']\n",
    "if \"media\" not in base_config['data_loader']['args']['data_dir']:\n",
    "    base_config['data_loader']['args']['data_dir'] = \"/media/brandon/SSD\" + base_config['data_loader']['args']['data_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = util.get_instance(models, 'model', base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_base = torch.load(base_model_cpt)\n",
    "state_dict_base = checkpoint_base['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_all_model:\n",
    "    model_base.load_state_dict(state_dict_base)\n",
    "    model_base.eval()\n",
    "    \n",
    "    base_target_class = list(np.arange(base_config['n_class']))\n",
    "    base_size_per_class = 3\n",
    "\n",
    "    base_data_loader = getattr(data_loaders, base_config['data_loader']['type'])(\n",
    "        base_config['data_loader']['args']['data_dir'],\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        validation_split=0.0,\n",
    "        training=False,\n",
    "        num_workers=2,\n",
    "        size_per_class=base_size_per_class,\n",
    "        target_class=base_target_class,\n",
    "        unknown=True\n",
    "    )\n",
    "    \n",
    "    base_loss_fn = getattr(loss_functions, base_config['loss'])\n",
    "\n",
    "    base_config['metrics'] = [\"pred_acc\"]\n",
    "\n",
    "    base_metrics = [getattr(metric_functions, met) for met in base_config['metrics']]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_metrics = torch.zeros(len(base_metrics))\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(tqdm(base_data_loader)):\n",
    "                one_hot_target = torch.eye(len(base_target_class))[target]\n",
    "\n",
    "#                 if \"cifar\" in task:\n",
    "#                     plt.figure(figsize=[10,10])\n",
    "\n",
    "#                     for index, image in enumerate(data):\n",
    "#                         plt.subplot(len(data)/ base_size_per_class, base_size_per_class, index+1)\n",
    "#                         plt.imshow((np.moveaxis(image.numpy(), 0, -1) * 255).astype(np.uint8))\n",
    "#                         plt.axis('off')\n",
    "#                         plt.title(target[index].item())\n",
    "\n",
    "                output = model_base(data)\n",
    "\n",
    "                # computing loss, metrics on test set\n",
    "                loss = base_loss_fn(output, one_hot_target)\n",
    "#                 loss = base_loss_fn(output, target)\n",
    "                batch_size = data.shape[0]\n",
    "                total_loss += loss.item() * batch_size\n",
    "                for i, metric in enumerate(base_metrics):\n",
    "                    total_metrics[i] += metric(output, target) * batch_size\n",
    "\n",
    "    n_samples = len(base_data_loader.sampler)\n",
    "    log = {'loss': total_loss / n_samples}\n",
    "    log.update({met.__name__ : total_metrics[i].item() / n_samples for i, met in enumerate(base_metrics)})\n",
    "\n",
    "    test_result_str = 'TEST RESULTS\\n'\n",
    "    for key, val in log.items():\n",
    "        test_result_str += ('\\t' + str(key) + ' : ' + str(val) + '\\n')\n",
    "\n",
    "    cp.print_progress(test_result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_config = {}\n",
    "fine_tuned_models = {}\n",
    "fine_tuned_state_dicts = {}\n",
    "\n",
    "for eval_target in eval_target_class:\n",
    "    cpt = torch.load(cpts[eval_target])\n",
    "    fine_tuned_config[eval_target] = cpt['config']\n",
    "    \n",
    "    if \"media\" not in fine_tuned_config[eval_target]['data_loader']['args']['data_dir']:\n",
    "        fine_tuned_config[eval_target]['data_loader']['args']['data_dir'] = \"/media/brandon/SSD\" + fine_tuned_config[eval_target]['data_loader']['args']['data_dir']\n",
    "    \n",
    "    fine_tuned_models[eval_target] = util.get_instance(models, 'model', fine_tuned_config[eval_target])\n",
    "    fine_tuned_models[eval_target].swap_fc(len(fine_tuned_config[eval_target]['target_class']) + 1)\n",
    "    fine_tuned_state_dicts[eval_target] = cpt['state_dict']\n",
    "    \n",
    "#     print(eval_target)\n",
    "#     pprint.pprint(fine_tuned_config[eval_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_all_model:\n",
    "    for eval_target in eval_target_class:\n",
    "        model = fine_tuned_models[eval_target]\n",
    "        config = fine_tuned_config[eval_target]\n",
    "        sd = fine_tuned_state_dicts[eval_target]\n",
    "        \n",
    "        model.load_state_dict(sd)\n",
    "        model.eval()\n",
    "        \n",
    "        size_per_class = 20\n",
    "        fine_tuned_target_class = [eval_target]\n",
    "        \n",
    "        fine_tuned_data_loader = getattr(data_loaders, config['data_loader']['type'])(\n",
    "            config['data_loader']['args']['data_dir'],\n",
    "            batch_size=512,\n",
    "            shuffle=False,\n",
    "            validation_split=0.0,\n",
    "            training=False,\n",
    "            num_workers=2,\n",
    "            size_per_class=size_per_class,\n",
    "            target_class=fine_tuned_target_class,\n",
    "            unknown=True\n",
    "        )\n",
    "        \n",
    "\n",
    "        loss_fn = getattr(loss_functions, config['loss'])\n",
    "\n",
    "        config['metrics'] = [\"pred_acc\"]\n",
    "\n",
    "        metrics = [getattr(metric_functions, met) for met in config['metrics']]\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_metrics = torch.zeros(len(base_metrics))\n",
    "\n",
    "#         pprint.pprint(fine_tuned_data_loader.dataset.classes)\n",
    "#         pprint.pprint(fine_tuned_data_loader.dataset.keyword_audios)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(tqdm(fine_tuned_data_loader)):\n",
    "                one_hot_target = torch.eye(len(fine_tuned_target_class) + 1)[target]\n",
    "\n",
    "#                 if \"cifar\" in task:\n",
    "#                     plt.figure(figsize=[15,15])\n",
    "\n",
    "#                     for index, image in enumerate(data):\n",
    "#                         plt.subplot(len(data)/ size_per_class, size_per_class, index+1)\n",
    "#                         plt.imshow((np.moveaxis(image.numpy(), 0, -1) * 255).astype(np.uint8))\n",
    "#                         plt.axis('off')\n",
    "#                         plt.title(target[index].item())\n",
    "\n",
    "                output = model(data)\n",
    "\n",
    "                # computing loss, metrics on test set\n",
    "                loss = loss_fn(output, one_hot_target)\n",
    "#                 loss = base_loss_fn(output, target)\n",
    "                batch_size = data.shape[0]\n",
    "                total_loss += loss.item() * batch_size\n",
    "                for i, metric in enumerate(metrics):\n",
    "                    total_metrics[i] += metric(output, target) * batch_size\n",
    "\n",
    "        n_samples = len(fine_tuned_data_loader.sampler)\n",
    "        log = {'loss': total_loss / n_samples}\n",
    "        log.update({met.__name__ : total_metrics[i].item() / n_samples for i, met in enumerate(metrics)})\n",
    "\n",
    "        test_result_str = 'TEST RESULTS\\n'\n",
    "        for key, val in log.items():\n",
    "            test_result_str += ('\\t' + str(key) + ' : ' + str(val) + '\\n')\n",
    "\n",
    "        cp.print_progress(test_result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_config = torch.load(base_model_cpt)['config']\n",
    "combined_model = util.get_instance(models, 'model', combined_config)\n",
    "combined_checkpoint = torch.load(base_model_cpt)\n",
    "combined_state_dict = checkpoint_base['state_dict']\n",
    "\n",
    "combined_model.load_state_dict(combined_state_dict)\n",
    "\n",
    "if \"media\" not in combined_config['data_loader']['args']['data_dir']:\n",
    "        combined_config['data_loader']['args']['data_dir'] = \"/media/brandon/SSD\" + combined_config['data_loader']['args']['data_dir']\n",
    "        \n",
    "# print('base')\n",
    "# for k,v in model_base.named_parameters():\n",
    "#     print(k, torch.mean(v).item())\n",
    "\n",
    "# print('old')\n",
    "# for k,v in model_1.named_parameters():\n",
    "#     print(k, torch.mean(v).item())\n",
    "    \n",
    "# print('old2')\n",
    "# for k,v in model_2.named_parameters():\n",
    "#     print(k, torch.mean(v).item())\n",
    "    \n",
    "# print('combined')\n",
    "# for k,v in model.named_parameters():\n",
    "#     print(k, torch.mean(v).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = []\n",
    "bias_list = []\n",
    "\n",
    "for eval_target in eval_target_class:\n",
    "    model = fine_tuned_models[eval_target]\n",
    "    config = fine_tuned_config[eval_target]\n",
    "    weight_list.append(fine_tuned_state_dicts[eval_target][\"fc.weight\"][0])\n",
    "    bias_list.append(fine_tuned_state_dicts[eval_target][\"fc.bias\"][0])\n",
    "#     weight_list.append(fine_tuned_state_dicts[eval_target][\"fc.weight\"][1])\n",
    "#     bias_list.append(fine_tuned_state_dicts[eval_target][\"fc.bias\"][1])\n",
    "\n",
    "weight = torch.stack(weight_list)\n",
    "bias = torch.stack(bias_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(36, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(60, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(72, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(84, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (6): BasicBlock(\n",
       "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(96, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (7): BasicBlock(\n",
       "        (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(108, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BasicBlock(\n",
       "        (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(120, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (9): BasicBlock(\n",
       "        (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(132, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (10): BasicBlock(\n",
       "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(144, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (11): BasicBlock(\n",
       "        (bn1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(156, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans1): TransitionBlock(\n",
       "    (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv1): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (block2): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(168, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(180, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(192, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(204, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (bn1): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(216, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (bn1): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(228, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (6): BasicBlock(\n",
       "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(240, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (7): BasicBlock(\n",
       "        (bn1): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(252, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BasicBlock(\n",
       "        (bn1): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(264, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (9): BasicBlock(\n",
       "        (bn1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(276, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (10): BasicBlock(\n",
       "        (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(288, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (11): BasicBlock(\n",
       "        (bn1): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(300, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans2): TransitionBlock(\n",
       "    (bn1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (conv1): Conv2d(312, 312, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (block3): DenseBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(312, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (bn1): BatchNorm2d(324, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(324, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (bn1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(336, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (bn1): BatchNorm2d(348, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(348, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (bn1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(360, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (bn1): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(372, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (6): BasicBlock(\n",
       "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(384, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (7): BasicBlock(\n",
       "        (bn1): BatchNorm2d(396, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(396, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BasicBlock(\n",
       "        (bn1): BatchNorm2d(408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(408, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (9): BasicBlock(\n",
       "        (bn1): BatchNorm2d(420, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(420, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (10): BasicBlock(\n",
       "        (bn1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(432, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (11): BasicBlock(\n",
       "        (bn1): BatchNorm2d(444, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv1): Conv2d(444, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(456, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (fc): Linear(in_features=456, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_id = combined_model.swap_fc(len(eval_target_class))\n",
    "combined_model.fc.weight = torch.nn.Parameter(weight)\n",
    "combined_model.fc.bias = torch.nn.Parameter(bias)\n",
    "\n",
    "combined_model.to(torch.device('cpu'))\n",
    "combined_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_loss_fn = getattr(loss_functions, combined_config['loss'])\n",
    "\n",
    "combined_config['metrics'] = [\"pred_acc\"]\n",
    "\n",
    "combined_metrics = [getattr(metric_functions, met) for met in combined_config['metrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "< Dataset Summary >\n",
      "\tseed\t: 10\n",
      "\t 0 - apple \t: 0  ( 100 )\n",
      "\t 1 - aquarium_fish \t: 1  ( 100 )\n",
      "\t 2 - baby \t: 2  ( 100 )\n",
      "\t 3 - bear \t: 3  ( 100 )\n",
      "\t 4 - beaver \t: 4  ( 100 )\n",
      "\t 5 - bed \t: 5  ( 100 )\n",
      "\t 6 - bee \t: 6  ( 100 )\n",
      "total data size :  700\n"
     ]
    }
   ],
   "source": [
    "combined_size_per_class = 100\n",
    "combined_data_loader = getattr(data_loaders, combined_config['data_loader']['type'])(\n",
    "    combined_config['data_loader']['args']['data_dir'],\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    validation_split=0.0,\n",
    "    training=False,\n",
    "    num_workers=2,\n",
    "    size_per_class=combined_size_per_class,\n",
    "    target_class=eval_target_class,\n",
    "    unknown=False,\n",
    "    seed=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:15<00:15, 15.70s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:21<00:00, 10.74s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "[ PROGRESS ] ::  TEST RESULTS\n",
      "\tloss : 1.1529671798433576\n",
      "\tpred_acc : 0.7871428571428571\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0.0\n",
    "total_metrics = torch.zeros(len(combined_metrics))\n",
    "\n",
    "# pprint.pprint(fine_tuned_data_loader.dataset.classes)\n",
    "# pprint.pprint(fine_tuned_data_loader.dataset.keyword_audios)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(tqdm(combined_data_loader)):\n",
    "        one_hot_target = torch.eye(len(eval_target_class))[target]\n",
    "\n",
    "#         if \"cifar\" in task:\n",
    "#             plt.figure(figsize=[15,15])\n",
    "\n",
    "#             print(len(data))\n",
    "#             for index, image in enumerate(data):\n",
    "#                 plt.subplot((len(data)/ combined_size_per_class)+1, combined_size_per_class, index+1)\n",
    "#                 plt.imshow((np.moveaxis(image.numpy(), 0, -1) * 255).astype(np.uint8))\n",
    "#                 plt.axis('off')\n",
    "#                 plt.title(target[index].item())\n",
    "\n",
    "        output = combined_model(data)\n",
    "\n",
    "        # computing loss, metrics on test set\n",
    "        loss = combined_loss_fn(output, one_hot_target)\n",
    "#                 loss = combined_loss_fn(output, target)\n",
    "        batch_size = data.shape[0]\n",
    "        total_loss += loss.item() * batch_size\n",
    "        for i, metric in enumerate(combined_metrics):\n",
    "            total_metrics[i] += metric(output, target) * batch_size\n",
    "\n",
    "n_samples = len(combined_data_loader.sampler)\n",
    "log = {'loss': total_loss / n_samples}\n",
    "log.update({met.__name__ : total_metrics[i].item() / n_samples for i, met in enumerate(combined_metrics)})\n",
    "\n",
    "test_result_str = 'TEST RESULTS\\n'\n",
    "for key, val in log.items():\n",
    "    test_result_str += ('\\t' + str(key) + ' : ' + str(val) + '\\n')\n",
    "\n",
    "cp.print_progress(test_result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
