\documentclass{article}
\usepackage[nonatbib]{nips_2016}

\usepackage[breaklinks=true,letterpaper=true,colorlinks,citecolor=black,bookmarks=false]{hyperref}

\usepackage{amsthm}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}

\usepackage[sort&compress,numbers]{natbib}
\usepackage[normalem]{ulem}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
%\usepackage{subfig}

\graphicspath{{../fig/}}

\usepackage{tikz}
\usepackage{tkz-tab}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{arrow} = [very thick,->,>=stealth]

\usepackage{cleveref}
\usepackage{setspace}
\usepackage{wrapfig}
%\usepackage[ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage[noend,linesnumbered]{algorithm2e}

\usepackage[disable]{todonotes}

\usepackage{algpseudocode}

\usepackage{booktabs}
\usepackage{multirow}


\title{Dynamic Model Construction for Efficient Classification}

\author{
	Jaejun Lee \\
	School of Computer Science\\
	University of Waterloo\\
	Waterloo, ON, N2L 3G1 \\
	\texttt{j474lee@uwaterloo.ca} \\
}

\begin{document}
\maketitle

\begin{abstract}

One of the major drawback of neural network in the domain of classification is that retraining is unavoidable when the target set of class changes. For this reason, networks are often designed to be wide and deep. However, this leads to increase the necessary computations which has a direct impact on the efficiency of the model. In this work, I study how different combination of loss function and last activation function affects the classification output and present a new way to add or remove a class from target class set without retraining. With this technique, a model can be adjusted to classify any combination of target classes and the minimal resource usage is guaranteed as the adjusted model involves the same amount of computations as the model trained to classify the same set of class explicitly.

\end{abstract}

\section{Introduction}

Current issues


In this section you are going to present a brief background and motivation of your project. Why is it interesting/significant? How does it relate to the course?

\section{Related Works}

The main challenge with this problem is on model flexibility and resource usage. Even though these two aspects are quite related, they are generally considered as two separate problem. As a result, this problem can be categorized differently depending on which direction one approaches. The three most related domains are: ensemble learning, multi-task learning, and transfer learning

\subsection{Ensemble Learning}
Ensemble learning refers to combining outputs from multiple models to generate a single output. Ensemble learning is a common technique in the field of machine learning. The most famous techniques include voting, weighting, bagging and boosting~\cite{dietterich2000ensemble, breiman1996bagging, freund1996experiments}. Even though ensemble learning has clear advantage of being easy to implement, it assumes that the models participating have different architecture. As a result, ensemble learning often requires significant computations as each model processes the given input independently.

\subsection{Multi-task Learning}
On the other hand, multi-task learning are considered to be more relevant as it involves combining multiple models into a single model. The key assumption is that tasks that each model is designed for, are related. Therefore, sharing information throughout training will lead to increase in performance for each task. There are numbers of different techniques varying on how to share and how much to share among tasks. Two most common approaches are parameter sharing and feature sharing~\cite{ruder2017overview, Caruana1993MultitaskLA, duong2015low, lu2017fully}. Even with the similarity in architecture, multi-task learning is still quite different from our problem, as it still assumes the models for each task have different architecture.

\subsection{Transfer Learning}

Transfer learning shares the same assumption with multi-task learning; two tasks are related and knowledge share can improve the performance. However, the key difference between two problem is that transfer learning are designed to share the same model for different tasks. First, the model is pre-trained with different tasks and learns to select important features. Then the model is fine-tuned for the target task, putting all of its effort to achieve the target task with selected features~\cite{yosinski2014transferable}. It is found that transfer learning is quite powerful and applicable in various domains~\cite{raina2007self, egan2004effects, glorot2011domain}.

\section{Proposed Works}

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{.3\linewidth}
%         \centering
%         \includegraphics[scale=0.48,trim={0mm 1.6mm 0mm 0mm},clip]{learning_rate.png}
%     \end{subfigure}%
%     \begin{subfigure}[b]{.3\linewidth}
%         \centering
%         \includegraphics[scale=0.48,trim={0mm 1.6mm 0mm 0mm},clip]{epoch.png}
%     \end{subfigure}
%     \begin{subfigure}[b]{.3\linewidth}
%         \centering
%         \includegraphics[scale=0.48,trim={0mm 1.6mm 0mm 0mm},clip]{epoch.png}
%     \end{subfigure}
%     \caption{Change in accuracy while varying the number of epochs and learning rate, along with 95\% confidence interval (shaded).}
%     \label{figure:change}
% \end{figure*}

\subsection{Class-level Dynamic Model Construction}

Among various techniques for multi-task learning, one of the successful approach is to use separate fully-connected layer for each task~\cite{huang2016mtnet, girshick2015fast, long2017learning}. After training, if one of the task is no longer necessary, corresponding fully-connected layers are removed saving computations during inference.

In the case of transfer learning, it is found that a model can perform better when it is pre-trained with different task. Since it is using the same architecture, we can clearly see that there is no additional computation for an inference.

Inspired by thewe two techniques, I propose class-level dynamic model construction (CLDMC). CLDMC can be achieved as following:

\begin{enumerate}
	\item pre-train a model with multi-class data
	\item freeze the model parameters and replace the last full-connected layer
	\item fine-tune the last layer to classify each class against all the other classes
	\item create a mapping from class to weights, where the weights in the mapping are the weights from the fine-tuned fully-connected layer which generate the output for the target class (Note that the weights for the negative class is dropped)
	\item at the time of the inference, the last fully-connected layer is constructed dynamically by loading weights for each class from the mapping
\end{enumerate}

Recall that the successful solution should satisfy the following conditions:
\begin{enumerate}
	\item dynamic addition and removal of a class must be possible
	\item minimal computation is desired for an inference
\end{enumerate}

With CLDMC, addition of a class is very simply as it can be achieved by adding a new connections in the fully-connected layer with the corresponding weights. Unlike multi-task learning, where retraining of the whole model is unavoidable in the case of task addition, CLDMC only requires fine-tuned weights of for the desired class. Since the weights for each class are obtained from independent fine-tuning (step 3, 4), we do not need to retrain the weights for the existing classes. Similarly, removal of a class can be achieved by excluding the unnecessary class weights during the model construction in step 5.

Note that the constructed model has the same amount of conputation as the baseline model which is designed and trained explcitly to detact the same set of class. In other words, there is no cost for adding or removing a class with CLDMC, satisfying the second criterion.

\section{Class-level Transfer Learning(required condition)}

Unfortunately, due to the dynamic construction of the last layer, specific setting is required to preserve the accuracy for the constructed model

\begin{table*}[t]
    \centering
    \small
    \scalebox{0.9}{
	    \begin{tabular}{ccccccccccccc}
	        \toprule[1pt]
	        \multirow{2}{*}{\raisebox{-3\heavyrulewidth}{\bf Loss function}} &
	        \multirow{2}{*}{\raisebox{-3\heavyrulewidth}{\bf Base model}} &
        	\multirow{2}{*}{\raisebox{-3\heavyrulewidth}{\bf Combined model}} &
        	\multicolumn{10}{c}{\bf Fine tuned accuracy per class } \\
        	\cmidrule(lr){4-13}
			& & & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
	        \midrule
			softmax + nll & 91.4 & 82.41 & 98.5 & 98.9 & 96.3 & 96.5 & 96.2 & 94.8 & 98.0 & 96.3 & 95.1 & 93.0 \\
			softmax + bce & 92.0 & 82.74 & 98.4 & 98.9 & 96.2 & 96.3 & 96.3 & 94.8 & 98.0 & 94.4 & 94.3 & 93.0 \\
			sigmoid + bce & 92.2 & 84.24 & 98.5 & 99.0 & 96.3 & 96.6 & 96.7 & 95.2 & 98.0 & 96.1 & 95.2 & 93.8 \\
	        \bottomrule[1pt]
	    \end{tabular}
    }
    \caption{Accuracy with different loss function}
    \label{table:accuracy}
\end{table*}


% & \multirow{2}{*}{\raisebox{-3\heavyrulewidth}{\bf Device}} &
        % \multirow{2}{*}{\raisebox{-3\heavyrulewidth}{\bf Processor}} &
        % \multirow{2}{*}{\raisebox{-3\heavyrulewidth}{\bf Platform}} &
        % \multicolumn{2}{c}{\bf \texttt{res8} } &
        % \multicolumn{2}{c}{\bf \texttt{res8-narrow} } \\
        % \cmidrule(lr){5-6}
        % \cmidrule(lr){7-8}
        % & & & & Lat. (ms) & Acc. (\%) & Lat. (ms) & Acc. (\%) \\
        % \midrule
        % \hspace{1mm}\multirow{5}{*}{GPU} & Desktop & GTX 1080 Ti & PyTorch  & 1 & 94.34 & 1 & 91.16 \\
        % & Desktop & GTX 1080 Ti & Firefox & 12 & 94.06 & 10 & 90.91 \\
        % & Macbook Pro (2017) & Intel Iris Plus 650 & Firefox  & 29 & 93.99 & 15 & 90.78 \\
        % & Macbook Air (2013) & Intel HD 6000 & Firefox  & 34 & 93.99 & 19 & 90.78 \\
        % & Galaxy S8 (2017) & Adreno 540 & Firefox & 60 & 94.06 & 43 & 88.96 \\
        % \midrule
        % \hspace{1mm}\multirow{6}{*}{CPU}
        % & Desktop & i7-4790k (quad) & PyTorch  & 10 & 94.30 & 2 & 91.16 \\
        % & Macbook Pro (2017) & i5-7287U (quad) & PyTorch  & 12 & 94.15 & 3 & 91.16 \\
        % & Desktop & i7-4790k (quad) & Firefox  & 371 & 94.06 & 94 & 90.91 \\
        % & Macbook Pro (2017) & i5-7287U (quad) & Firefox  & 361 & 93.99 & 107 & 90.78 \\
        % & Macbook Air (2013) & i5-4260U (dual) & Firefox  & 485 & 93.99 & 115 & 90.78 \\
        % & Galaxy S8 (2017) & Snapdragon 835 (octa) & Firefox & 1105 & 94.06 & 265 & 88.96 \\
















\subsection{results}
Perform an initial review of relevant literature. Has your problem, or one of similar nature, been considered before? By whom? What are the differences or limitations (if any)?

\subsection{computational efficiency}
Perform an initial review of relevant literature. Has your problem, or one of similar nature, been considered before? By whom? What are the differences or limitations (if any)?

\section{Conclusion}
In this section please concisely describe what you are going to achieve in this project. E.g., formulate your problem precisely (mathematically), present the technical challenges (if any), discuss the tools or datasets that you will build on, state your goals, and come up with a plan for evaluation.

For your own sake, you might want to lay out a time line, so that you can keep a good track of your project.

\newpage

\section*{Acknowledgement}
Thank people who have helped or influenced you in this project.

\nocite{*}

\bibliographystyle{unsrtnat}
\bibliography{citation}

\end{document}
